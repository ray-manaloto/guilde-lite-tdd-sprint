{
  "metadata": {
    "description": "Comprehensive library of 76 AI research engineering skills enabling autonomous AI research from hypothesis to experimental verification",
    "version": "1.0.0"
  },
  "name": "ai-research-skills",
  "owner": {
    "email": "zechen@orchestra-research.com",
    "name": "Orchestra Research"
  },
  "plugins": [
    {
      "description": "LLM architectures and implementations including LitGPT, Mamba, NanoGPT, and RWKV. Use when implementing, training, or understanding transformer and alternative architectures.",
      "name": "model-architecture",
      "skills": [
        "./01-model-architecture/litgpt",
        "./01-model-architecture/mamba",
        "./01-model-architecture/nanogpt",
        "./01-model-architecture/rwkv"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "Text tokenization for LLMs including HuggingFace Tokenizers and SentencePiece. Use when training custom tokenizers or handling multilingual text.",
      "name": "tokenization",
      "skills": [
        "./02-tokenization/huggingface-tokenizers",
        "./02-tokenization/sentencepiece"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "LLM fine-tuning frameworks including Axolotl, LLaMA-Factory, PEFT, and Unsloth. Use when fine-tuning models with LoRA, QLoRA, or full fine-tuning.",
      "name": "fine-tuning",
      "skills": [
        "./03-fine-tuning/axolotl",
        "./03-fine-tuning/llama-factory",
        "./03-fine-tuning/peft",
        "./03-fine-tuning/unsloth"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "Neural network interpretability tools including TransformerLens, SAELens, NNSight, and pyvene. Use when analyzing model internals, finding circuits, or understanding how models compute.",
      "name": "mechanistic-interpretability",
      "skills": [
        "./04-mechanistic-interpretability/nnsight",
        "./04-mechanistic-interpretability/pyvene",
        "./04-mechanistic-interpretability/saelens",
        "./04-mechanistic-interpretability/transformer-lens"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "Data curation and processing at scale including NeMo Curator and Ray Data. Use when preparing training datasets or processing large-scale data.",
      "name": "data-processing",
      "skills": [
        "./05-data-processing/nemo-curator",
        "./05-data-processing/ray-data"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "RLHF and preference alignment including TRL, GRPO, OpenRLHF, and SimPO. Use when aligning models with human preferences or training reward models.",
      "name": "post-training",
      "skills": [
        "./06-post-training/grpo-rl-training",
        "./06-post-training/openrlhf",
        "./06-post-training/simpo",
        "./06-post-training/trl-fine-tuning"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "AI safety and content moderation including Constitutional AI, LlamaGuard, and NeMo Guardrails. Use when implementing safety filters or content moderation.",
      "name": "safety-alignment",
      "skills": [
        "./07-safety-alignment/constitutional-ai",
        "./07-safety-alignment/llamaguard",
        "./07-safety-alignment/nemo-guardrails"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "Multi-GPU and multi-node training including DeepSpeed, PyTorch FSDP, Accelerate, Megatron-Core, PyTorch Lightning, and Ray Train. Use when training large models across GPUs.",
      "name": "distributed-training",
      "skills": [
        "./08-distributed-training/accelerate",
        "./08-distributed-training/deepspeed",
        "./08-distributed-training/megatron-core",
        "./08-distributed-training/pytorch-fsdp",
        "./08-distributed-training/pytorch-lightning",
        "./08-distributed-training/ray-train"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "GPU cloud and compute orchestration including Modal, Lambda Labs, and SkyPilot. Use when deploying training jobs or managing GPU resources.",
      "name": "infrastructure",
      "skills": [
        "./09-infrastructure/lambda-labs",
        "./09-infrastructure/modal",
        "./09-infrastructure/skypilot"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "Model optimization and quantization including Flash Attention, bitsandbytes, GPTQ, AWQ, GGUF, and HQQ. Use when reducing memory, accelerating inference, or quantizing models.",
      "name": "optimization",
      "skills": [
        "./10-optimization/awq",
        "./10-optimization/bitsandbytes",
        "./10-optimization/flash-attention",
        "./10-optimization/gguf",
        "./10-optimization/gptq",
        "./10-optimization/hqq"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "LLM benchmarking and evaluation including lm-evaluation-harness, BigCode Evaluation Harness, and NeMo Evaluator. Use when benchmarking models or measuring performance.",
      "name": "evaluation",
      "skills": [
        "./11-evaluation/bigcode-evaluation-harness",
        "./11-evaluation/lm-evaluation-harness",
        "./11-evaluation/nemo-evaluator"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "Production LLM inference including vLLM, TensorRT-LLM, llama.cpp, and SGLang. Use when deploying models for production inference.",
      "name": "inference-serving",
      "skills": [
        "./12-inference-serving/llama-cpp",
        "./12-inference-serving/sglang",
        "./12-inference-serving/tensorrt-llm",
        "./12-inference-serving/vllm"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "ML experiment tracking and lifecycle including Weights & Biases, MLflow, and TensorBoard. Use when tracking experiments or managing models.",
      "name": "mlops",
      "skills": [
        "./13-mlops/mlflow",
        "./13-mlops/tensorboard",
        "./13-mlops/weights-and-biases"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "LLM agent frameworks including LangChain, LlamaIndex, CrewAI, and AutoGPT. Use when building chatbots, autonomous agents, or tool-using systems.",
      "name": "agents",
      "skills": [
        "./14-agents/autogpt",
        "./14-agents/crewai",
        "./14-agents/langchain",
        "./14-agents/llamaindex"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "Retrieval-Augmented Generation including Chroma, FAISS, Pinecone, Qdrant, and Sentence Transformers. Use when building semantic search or document retrieval systems.",
      "name": "rag",
      "skills": [
        "./15-rag/chroma",
        "./15-rag/faiss",
        "./15-rag/pinecone",
        "./15-rag/qdrant",
        "./15-rag/sentence-transformers"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "Structured LLM outputs including DSPy, Instructor, Guidance, and Outlines. Use when extracting structured data or constraining LLM outputs.",
      "name": "prompt-engineering",
      "skills": [
        "./16-prompt-engineering/dspy",
        "./16-prompt-engineering/guidance",
        "./16-prompt-engineering/instructor",
        "./16-prompt-engineering/outlines"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "LLM application monitoring including LangSmith and Phoenix. Use when debugging LLM apps or monitoring production systems.",
      "name": "observability",
      "skills": [
        "./17-observability/langsmith",
        "./17-observability/phoenix"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "Vision, audio, and multimodal models including CLIP, Whisper, LLaVA, BLIP-2, Segment Anything, Stable Diffusion, and AudioCraft. Use when working with images, audio, or multimodal tasks.",
      "name": "multimodal",
      "skills": [
        "./18-multimodal/audiocraft",
        "./18-multimodal/blip-2",
        "./18-multimodal/clip",
        "./18-multimodal/llava",
        "./18-multimodal/segment-anything",
        "./18-multimodal/stable-diffusion",
        "./18-multimodal/whisper"
      ],
      "source": "./",
      "strict": false
    },
    {
      "description": "Advanced ML techniques including MoE Training, Model Merging, Long Context, Speculative Decoding, Knowledge Distillation, and Model Pruning. Use when implementing cutting-edge optimization or architecture techniques.",
      "name": "emerging-techniques",
      "skills": [
        "./19-emerging-techniques/knowledge-distillation",
        "./19-emerging-techniques/long-context",
        "./19-emerging-techniques/model-merging",
        "./19-emerging-techniques/model-pruning",
        "./19-emerging-techniques/moe-training",
        "./19-emerging-techniques/speculative-decoding"
      ],
      "source": "./",
      "strict": false
    }
  ]
}