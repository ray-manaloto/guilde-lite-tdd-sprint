# Paired Subagent Review (ceo-stakeholder)

Provider: anthropic
Model: claude-opus-4-5-20251101

---

# CEO-Stakeholder Review: Actionable Takeaways for Spec/Plan

Based on the deep-research summary, here are **7 actionable takeaways** with explicit risks and decisions to encode:

---

## 1. **Adopt Hybrid LLM-Led + Code-Led Orchestration**
- Use OpenAI Agents SDK's code-led mode for deterministic, auditable workflows (e.g., CI gates, compliance checks).
- Reserve LLM-led mode for open-ended tasks (user query routing, creative generation).
- **Decision to encode:** Define in spec which workflows are code-led vs. LLM-led. Default to code-led unless explicitly justified.
- **Risk:** LLM-led agents can behave unpredictably; require guardrails and structured outputs for all LLM-led paths.

---

## 2. **Implement Guardrails and Structured Outputs as Mandatory**
- All agent outputs must use JSON schema constraints (structured outputs) for downstream parsing.
- Input guardrails (prompt filters, approval steps) must be enabled for any user-facing or high-stakes agent.
- **Decision to encode:** No agent endpoint ships without a defined output schema and at least one input guardrail. Add schema validation to integration tests.
- **Risk:** Missing guardrails can allow prompt injection or malformed output. Mandate code review for guardrail config changes.

---

## 3. **Use LiteLLM for Anthropic (Claude) Routing with Explicit Prefix Convention**
- Route all Claude requests via LiteLLM using `anthropic/claude-*` model prefix.
- Rely on LiteLLM's automatic parameter mapping (e.g., `max_tokens`, `response_format` → `output_schema`).
- **Decision to encode:** Standardize model naming in config; never hardcode Anthropic base URLs—always proxy through LiteLLM.
- **Risk:** Anthropic API quirks (e.g., missing `max_tokens`) can cause silent failures. LiteLLM handles this, but log all param mappings for debuggability.

---

## 4. **Enable Full Passthrough and Cost Tracking for Anthropic**
- Use LiteLLM's Anthropic passthrough for batched and streaming endpoints (`/v1/messages`, `/v1/messages/batches`).
- Instrument cost tracking and logging at the LiteLLM proxy layer.
- **Decision to encode:** Require all Anthropic calls to flow through LiteLLM proxy; expose cost metrics in observability stack.
- **Risk:** Direct Anthropic calls bypass tracking. Enforce at network/config level; fail CI if direct calls detected.

---

## 5. **Pytest: Register and Gate on Custom Integration Markers**
- Define `@pytest.mark.integration` in `pyproject.toml` or `pytest.ini`.
- CI default: run `pytest -m "not integration"` for fast feedback; gate merge on `pytest -m integration` in a separate stage.
- **Decision to encode:** All integration tests must be marked; unregistered markers fail CI. Add a conftest hook to auto-skip integration tests unless explicitly requested.
- **Risk:** Unmarked integration tests slow down CI or escape gating. Add a lint/check for unmarked tests in `tests/integration/`.

---

## 6. **Separate Integration Test Folder + Marker Redundancy**
