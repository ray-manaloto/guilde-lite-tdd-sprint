# Deep Research Output

Model: o4-mini-deep-research

Prompt:
Deep research for planning: OpenAI Agents SDK orchestration best practices, LiteLLM usage for Anthropic routing, and pytest integration test marking (modern pytest + PEP guidance). Provide concise bullets and include source URLs when you cite guidance.

---

# OpenAI Agents SDK orchestration best practices  
- **Use the Agents SDK for orchestration:** it provides built-in primitives (Agent, Handoff, Guardrail, Session) that automate multi-step workflows, context, and tool management ([developers.openai.com](https://developers.openai.com/tracks/building-agents#:~:text=The%20Agents%20SDK%20uses%20a,few%20core%20primitives)) ([developers.openai.com](https://developers.openai.com/tracks/building-agents#:~:text=,stop%20the%20generation%20if%20required)).  
- **Choose LLM-led vs code-led mode:** LLM-led agents autonomously plan/delegate tasks (best for open-ended or dynamic tasks) while code-led orchestration lets you explicitly script each step (making workflows predictable) ([medium.com](https://medium.com/%40abdulkabirlive1/openai-agents-sdk-expert-level-guide-to-orchestration-44eeb67e1ec7#:~:text=1.%20LLM)). You can mix both for adaptability and stability ([medium.com](https://medium.com/%40abdulkabirlive1/openai-agents-sdk-expert-level-guide-to-orchestration-44eeb67e1ec7#:~:text=%2A%20LLM,Handoffs%20make%20your%20agents%20modular)).  
- **Leverage handoffs for modularity:** in LLM-led workflows, agents can call `handoff(...)` to delegate subtasks to specialist agents ([medium.com](https://medium.com/%40abdulkabirlive1/openai-agents-sdk-expert-level-guide-to-orchestration-44eeb67e1ec7#:~:text=In%20LLM,to%20delegate%20to%20other%20agents)). The SDK handles these handoff transitions and tool-calling loops automatically ([developers.openai.com](https://developers.openai.com/tracks/building-agents#:~:text=,stop%20the%20generation%20if%20required)).  
- **Add guardrails and structured outputs:** guard inputs (e.g. prompt filters or approval steps) to prevent unwanted instructions, and use structured-output schemas (JSON prompts) to constrain model responses to a fixed format ([developers.openai.com](https://developers.openai.com/tracks/building-agents#:~:text=User%20inputs)) ([developers.openai.com](https://developers.openai.com/tracks/building-agents#:~:text=A%20good%20practice%20is%20to,the%20output%20shape%20will%20be)). This ensures you can reliably parse outputs and avoid invalid answers.  
- **Monitor and optimize in production:** because LLMs can be unpredictable, instrument and track your agents (cost, latency, accuracy) and iterate on prompts/models.  Optimize for performance and cost (per OpenAI’s AI app development guidance) ([developers.openai.com](https://developers.openai.com/tracks/building-agents#:~:text=Optimizing%20for%20production)).  

# LiteLLM usage for Anthropic routing  
- **Route to Claude models via LiteLLM:** LiteLLM supports all Anthropic (Claude) models. Use the `anthropic/` prefix on model names (e.g. `model="anthropic/claude-3-5-sonnet-20240620"`) or replace the Anthropic base URL with `LITELLM_PROXY_BASE_URL/anthropic` in your HTTP calls ([docs.litellm.ai](https://docs.litellm.ai/docs/pass_through/anthropic_completion#:~:text=Just%20replace%20)) ([docs.litellm.ai](https://docs.litellm.ai/docs/providers/anthropic#:~:text=Provider%20Route%20on%20LiteLLM%20,name%3E.services.ai.azure.com%2Fanthropic)). This proxies requests through LiteLLM to Claude.  
- **Passthrough support with tracking:** LiteLLM’s Anthropic passthrough supports every Claude endpoint (e.g. `/v1/messages`, `/v1/messages/batches`) with full cost tracking, logging, and streaming enabled ([docs.litellm.ai](https://docs.litellm.ai/docs/pass_through/anthropic_completion#:~:text=Feature%20%20,%E2%9C%85)). You simply send requests to `http://<liteLLM>/anthropic/...` with your LiteLLM API key to leverage Anthropic’s models.  
- **Automatic Anthropic parameter mapping:** LiteLLM handles Claude’s quirks automatically. It injects a default `max_tokens` (4096) when missing, maps standard OpenAI params (like `stream`, `top_p`, `reasoning_effort`) to Anthropic’s API (e.g. mapping `reasoning_effort` into `output_config`), and converts OpenAI’s `response_format` into Anthropic’s `output_schema` for supported models ([docs.litellm.ai](https://docs.litellm.ai/docs/providers/anthropic#:~:text=Notes%3A)) ([docs.litellm.ai](https://docs.litellm.ai/docs/providers/anthropic#:~:text=When%20using%20structured%20outputs%20with,supported%20models%2C%20LiteLLM%20automatically)).  
- **Structured outputs support:** For Claude Sonnet 4.5 / Opus 4.1 models, LiteLLM adds the required `anthropic-beta: structured-outputs-2025-11-13` header and converts your JSON schema prompts into Claude’s structured-output format, so you get back JSON with a known schema ([docs.litellm.ai](https://docs.litellm.ai/docs/providers/anthropic#:~:text=When%20using%20structured%20outputs%20with,supported%20models%2C%20LiteLLM%20automatically)).  
- **Azure Foundry integration:** To call Claude via Azure, use the `azure/claude-…` prefix and set `api_base` to your Azure Claude endpoint. LiteLLM will append the correct `/v1/messages` suffix by default (or you can disable it with `LITELLM_ANTHROPIC_DISABLE_URL_SUFFIX`) ([docs.litellm.ai](https://docs.litellm.ai/docs/providers/anthropic#:~:text=Azure%20Foundry%20Support)) ([docs.litellm.ai](https://docs.litellm.ai/docs/providers/anthropic#:~:text=Custom%20API%20Base%E2%80%8B)).  
- **Anthropic Claude Code gateway:** Anthropic’s docs note you can configure LiteLLM as a Claude LLM gateway. Set `ANTHROPIC_AUTH_TOKEN` (or use a helper script) to a LiteLLM API key in your Claude Code settings; then all Claude requests go through your LiteLLM proxy ([docs.anthropic.com](https://docs.anthropic.com/en/docs/claude-code/llm-gateway#:~:text=)). (Anthropic provides this as an informational setup for centralized routing and tracking.)  

# pytest integration test marking (modern pytest + PEP guidance)  
- **Follow PEP8 naming:** Name test files and functions in lowercase snake_case (e.g. `test_my_feature.py`, `def test_works()`) and test classes in CapWords (e.g. `class TestFeature:`) ([stackoverflow.com](https://stackoverflow.com/questions/20513596/pep8-naming-convention-on-test-classes#:~:text=29)) ([docs.pytest.org](https://docs.pytest.org/latest/example/markers.html#:~:text=)). Pytest by default collects any file or function starting with `test_`.  
- **Use custom markers:** In `pytest.ini`, define a custom marker, e.g.  
  ```ini
  [pytest]
  markers =
      integration: marks an integration test (runs with network/db/etc)
  ```  
  Then tag each integration test with `@pytest.mark.integration`. This lets pytest recognize the marker and avoids warnings ([stackoverflow.com](https://stackoverflow.com/questions/66315234/exclude-some-tests-by-default#:~:text=%60,test%20that%20requires%20environment)).  
- **Selectively include/exclude:** By default, skip long-running/integration tests with a condition (e.g. using a pytest hook or `pytest -m`): for example, add a hook in `conftest.py` to auto-skip tests marked `integration` unless `-m integration` is given ([stackoverflow.com](https://stackoverflow.com/questions/66315234/exclude-some-tests-by-default#:~:text=conftest)) ([stackoverflow.com](https://stackoverflow.com/questions/66315234/exclude-some-tests-by-default#:~:text=now%20all%20tests%20marked%20as,are%20skipped%20unless%20you%20use)). Practically, run `pytest -m "not integration"` to run only unit tests or `pytest -m integration` to run them explicitly.  
- **Or separate test folders:** As an alternative to markers, you can put integration tests in a separate folder (e.g. `tests/integration/`). Then running `pytest tests/integration` executes only those, and `pytest tests` excludes them naturally ([stackoverflow.com](https://stackoverflow.com/questions/54898578/how-to-keep-unit-tests-and-integrations-tests-separate-in-pytest#:~:text=You%20can%20also%20structurally%20separate,Started%20With%20Testing%20in%20Python)). This structural separation (instead of always marking) is a modern pytest pattern.  
- **Register markers (PEP8/pytest):** Always register any custom markers in your config (or use the newer `pyproject.toml [tool.pytest.ini_options]`) to comply with modern pytest. Unsigned markers produce a warning (“PytestUnknownMarkWarning”), so listing them in `pytest.ini` or `pyproject.toml` is best practice ([stackoverflow.com](https://stackoverflow.com/questions/66315234/exclude-some-tests-by-default#:~:text=%60,test%20that%20requires%20environment)). This aligns with pytest’s recommendations for custom test categories.  

**Sources:** Best practices from official OpenAI docs and Medium articles ([developers.openai.com](https://developers.openai.com/tracks/building-agents#:~:text=The%20Agents%20SDK%20uses%20a,few%20core%20primitives)) ([medium.com](https://medium.com/%40abdulkabirlive1/openai-agents-sdk-expert-level-guide-to-orchestration-44eeb67e1ec7#:~:text=1.%20LLM)); LiteLLM documentation on Anthropic integration ([docs.litellm.ai](https://docs.litellm.ai/docs/providers/anthropic#:~:text=Provider%20Route%20on%20LiteLLM%20,name%3E.services.ai.azure.com%2Fanthropic)) ([docs.litellm.ai](https://docs.litellm.ai/docs/providers/anthropic#:~:text=When%20using%20structured%20outputs%20with,supported%20models%2C%20LiteLLM%20automatically)); and pytest documentation/StackOverflow guidelines on test naming and marking ([stackoverflow.com](https://stackoverflow.com/questions/20513596/pep8-naming-convention-on-test-classes#:~:text=29)) ([stackoverflow.com](https://stackoverflow.com/questions/66315234/exclude-some-tests-by-default#:~:text=%60,test%20that%20requires%20environment)).
