# Paired Subagent Review (research-scientist)

Provider: anthropic
Model: claude-opus-4-5-20251101

---

# Actionable Takeaways from Deep Research

## OpenAI Agents SDK Orchestration

1. **Adopt hybrid orchestration pattern** – Use LLM-led mode for dynamic/exploratory tasks and code-led mode for deterministic workflows. Encode a decision in spec: *default to code-led with explicit opt-in to LLM-led for specific agent types*.
   - **Risk**: LLM-led agents can produce unpredictable handoff chains. Mitigate by enforcing a max handoff depth and circuit-breaker timeout.

2. **Use SDK primitives (Agent, Handoff, Guardrail, Session)** – Don't reinvent context management. Let the SDK manage tool-calling loops and handoff transitions.
   - **Decision**: Standardize on `Session` objects for multi-turn conversations; avoid custom state stores unless SDK proves insufficient.

3. **Implement input guardrails and structured outputs** – Add prompt filters for injection defense and require JSON schema outputs for all agent responses that feed downstream logic.
   - **Risk**: Structured output schemas can fail silently if model drifts. Add runtime validation (Pydantic) on every response parse.

4. **Instrument from day one** – Track cost, latency, and accuracy per agent/model. Expose metrics endpoint or integrate with OpenTelemetry.
   - **Decision**: Include `opentelemetry-sdk` in dependencies; define span naming convention in spec (e.g., `agent.<agent_name>.<action>`).

---

## LiteLLM Anthropic Routing

5. **Use `anthropic/` model prefix for routing** – Configure LiteLLM proxy and call models as `anthropic/claude-3-5-sonnet-20240620`. Avoid direct Anthropic SDK calls to centralize auth, logging, and cost tracking.
   - **Decision**: All Anthropic calls MUST route through LiteLLM proxy; encode this as a linting rule or pre-commit check.

6. **Leverage automatic parameter mapping** – LiteLLM injects `max_tokens=4096` default and maps `response_format` → `output_schema`. Do not duplicate this logic in application code.
   - **Risk**: Implicit defaults can cause surprise cost spikes (4096 tokens on every call). Override with explicit, lower `max_tokens` where possible.

7. **Enable passthrough for full Claude API surface** – Use `/anthropic/v1/messages` passthrough endpoint for streaming, batches, and extended thinking if needed. Ensure `LITELLM_PROXY_BASE_URL` env var is set in all environments.
   - **Decision**: Document required env vars in `.env.example` and fail fast on startup if missing.

---

## Pytest Integration Test Gating

8. **Define `@pytest.mark.integration` marker and gate in CI** – Register marker in `pyproject.toml` under `[tool.pytest.ini_options]`. Default CI runs `pytest -m "not integration"`; nightly or pre-deploy runs `pytest -m integration`.
   - **Decision**: Encode marker registration in spec; add `conftest.py` hook to auto-skip integration tests unless explicitly requested.
   - **Risk**: Unmarked integration tests slip into unit suite and cause flaky CI. Mitigate with a pre-commit hook that checks for network/db fixtures → requires marker.

---

## Risks & Decisions to Encode in Spec/Plan

|
