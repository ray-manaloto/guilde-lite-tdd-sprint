# Design Notes (data-scientist)

## Confidence/Vagueness Gate Logic
- Gate is driven by structured output fields, not heuristic parsing.
- Required JSON output fields for planning interviews:
  - `confidence`: low | medium | high
  - `clarifying_questions`: string[]
  - `assumptions`: string[]
  - `dependencies`: string[]
  - `acceptance_criteria`: string[]
- Low confidence triggers manual confirmation before moving to next phase.

Suggested gate rules:
- `confidence = low` if any of the following are true:
  - `acceptance_criteria` is empty or missing.
  - `dependencies` has unresolved items (prefix with "UNKNOWN:").
  - `clarifying_questions` length > 0.
- `confidence = medium` if criteria present but assumptions remain.
- `confidence = high` only when criteria + dependencies are explicit and no questions remain.

## Output Schema (Planning)
- The schema used for `--output-format json` should include:
  - `spec_markdown` and `plan_markdown`
  - `confidence` and gate metadata
  - `sources` (optional) for research citations

## Telemetry + Metrics
- Capture usage, latency, and model id per run via Agents SDK usage.
- Store cost metrics in DB mirror (optional if model provider exposes cost).
- Record gate outcomes in artifact state or DB mirror for auditability.

## Validation Strategy
- Add schema validation tests for JSON outputs.
- Add a test that fails if `confidence=low` does not require confirmation.
- Maintain `pytest -m integration` as the canonical integration gate.

## Data Retention
- Keep artifact content canonical in `conductor/`.
- DB mirror should store:
  - artifact path
  - content hash
  - model id
  - usage stats
