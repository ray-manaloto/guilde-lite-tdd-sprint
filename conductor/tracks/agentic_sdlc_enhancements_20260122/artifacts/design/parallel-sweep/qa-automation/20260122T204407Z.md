## Design Decisions
- **Phase 2 scope**: add PR smoke gates, nightly full E2E, and true integration tests (no mocks) with AI-assisted review and CI observability.
- **Smoke PR gates**: run a minimal, deterministic E2E subset for top critical flows (login, sprint creation, plan run). Keep runtime <10–15 minutes. Fail-fast on environment setup issues.
- **Nightly full E2E**: run full suite with optional multi-browser matrix, plus visual and a11y checks. Use retries + trace capture for diagnostics, not to mask flakiness.
- **Integration tests without mocks**: stand up real DB + service dependencies in CI. Block merge if contract/integration tests fail. Use seed data fixtures and reset between runs.
- **Selectors and stability**: require `data-testid` on critical UI elements; use page objects and explicit waits to reduce flakiness.
- **Hooks & automation**: use hook-driven checks for lint/format/unit smoke; run heavy tests only in PR gate or nightly to keep edit loops fast.
- **AI-assisted review**: adopt an evaluator pattern—one agent generates change review/test suggestions, another verifies and summarizes risk. Use CI observability agent to triage failures and produce summaries for reviewers.
- **Context persistence**: maintain a QA notes file (`QA_NOTES.md`) capturing flaky tests, known risks, and test data dependencies for reuse by agents and humans.

## Deliverables
- **PR Smoke Gate** workflow: CI job running E2E smoke flows + unit smoke.
- **Nightly Full E2E** workflow: full suite with visual + a11y, optional multi-browser.
- **Integration Test Harness**: docker-compose/infra-as-code for DB + services; data seeding; teardown.
- **Playwright baseline**: page objects, test data utilities, stable selectors, trace/snapshot configuration.
- **AI Review Step**: headless AI job that summarizes PR risk, test gaps, and regression hotspots.
- **QA Docs**: `QA_NOTES.md`, selector guidelines, flake triage SOP.

## Gates
- **PR Gate (required)**
  - Lint/format + unit smoke
  - E2E smoke flows (login, sprint creation, plan run)
  - Integration test subset (critical service paths)
  - AI reviewer summary posted to PR
- **Nightly Gate (required for release)**
  - Full E2E suite (all flows)
  - Visual regression + a11y checks
  - Full integration tests (real services)
  - AI observability summary for failures
- **Release Gate (manual)**
  - Human QA sign-off on nightly status and AI reviewer notes

## Risks/Dependencies
- **Flakiness risk**: E2E instability may cause false negatives; mitigated by stable selectors, explicit waits, and trace artifacts.
- **Environment parity**: integration tests require consistent DB/service versions; needs infra ownership and seed data governance.
- **Runtime constraints**: multi-browser nightly may exceed budget; requires capacity planning.
- **AI review quality**: evaluator agents may miss systemic bugs; requires periodic calibration and human spot checks.
- **Hook overuse**: running heavy tests on edit hooks can slow developer loop; must keep hooks lightweight.

## Open Questions
- Which **critical flows** constitute the definitive smoke set beyond login/sprint creation/plan run?
- What **service dependencies** are mandatory for “no-mock” integration tests?
- Should **multi-browser** be nightly-only or also weekly to control cost?
- What is the **acceptable flake rate** before quarantining tests?
- How will **AI reviewer output** be enforced—advisory or merge-blocking?