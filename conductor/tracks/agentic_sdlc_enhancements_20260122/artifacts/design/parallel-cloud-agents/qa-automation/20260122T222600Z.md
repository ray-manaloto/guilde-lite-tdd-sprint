### Design Decisions
- **Cloud test runners as subagents**: Use ephemeral Lambda/Fargate runners for QA subagent tasks (unit/integration/e2e), triggered by Conductor artifact changes (spec/plan diffs). Runners pull minimal context (spec.md, plan.md, relevant patches) and return **test reports + patch diffs**.
- **Event-driven orchestration**: Wire Conductor “Verify” stage to queue-based triggers; cap concurrency and enforce timeouts to avoid runaway test loops.
- **Sandbox + least privilege**: Run tests in isolated VPC subnets with restricted egress; IAM scoped to read-only repo + test artifacts bucket.
- **Observability & cost**: Emit OTel traces + CloudWatch logs per test run; tag all runs with Conductor workflow IDs. Use budgets/alerts and reserved concurrency for QA pools.
- **Artifact alignment**: QA output references existing `spec.md` and `plan.md` sections by anchor (no re-documentation); report diffs only for necessary changes.

### Deliverables
- **QA Subagent Runner**: Containerized test runner definition (Fargate/Lambda) with minimal input contract.
- **Test Report Artifact**: Standardized `verify-report.md` (linked to spec/plan sections) + machine-readable JSON summary.
- **Patch Artifacts**: Optional diffs for flaky test fixes or config updates.
- **Observability Dashboard**: Trace + token usage + duration metrics for QA runs.

### Gates
- **Spec Gate**: Test plan references spec anchors; missing anchors fail.
- **Plan Gate**: Plan includes test scope, data requirements, and environments; missing = block “Implement”.
- **Verify Gate**: 
  - All required suites pass (unit + integration + smoke e2e).
  - No critical regressions vs baseline (defined in plan).
  - Test reports linked to Conductor run ID.
- **Security Gate**: No secrets in logs/artifacts; IAM policies validated.

### Risks/Dependencies
- **Dependency on infra**: Requires VPC/subnet + IAM policies + artifact bucket.
- **Test data fragility**: External dependencies can cause flakiness; mitigate via mocks/fixtures.
- **Cost drift**: High concurrency may spike spend if gating not enforced.

### Open Questions
1. Which test suites are mandatory per repo stage (unit/integration/e2e)?
2. Preferred environment for integration tests (staging vs ephemeral)?
3. Required SLA for QA runs (max duration, retries)?
4. Where should QA artifacts be stored (S3, repo artifacts, Conductor storage)?