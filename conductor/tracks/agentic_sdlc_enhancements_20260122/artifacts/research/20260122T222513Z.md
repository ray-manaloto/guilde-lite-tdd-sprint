# Summary  
Cloud-hosted AI subagents can enhance a local AI agent’s capabilities by offloading heavy or risky tasks to AWS or other cloud providers. To design such a system, it’s critical to use robust orchestration patterns that coordinate multiple agents effectively, while enforcing strong security isolation to sandbox each subagent. Scheduling mechanisms (event triggers or workflows) should ensure tasks run in the right order and at the right time, with controls to manage concurrency and avoid runaway usage. Cost management (“**freedom with guardrails**”) is vital – using pay-per-use resources, setting budgets/limits, and optimizing resource choices prevents cloud expenses from spiraling ([aws.amazon.com](https://aws.amazon.com/blogs/compute/serverless-automated-cost-controls-part1/#:~:text=This%20improved%20agility%20also%20brings,freedom%2C%20but%20with%20financial%20constraints)). Equally important is observability: comprehensive logging, metrics, and tracing allow monitoring of subagents’ behavior (e.g. to catch infinite loops that could rack up costs ([medium.com](https://medium.com/%40acmopm/agentic-ai-observability-with-amazon-cloudwatch-transforming-enterprise-ai-monitoring-for-the-cc576b4258e8#:~:text=Real,monitoring%20and%20loop%20detection%20capabilities))). Secrets must be handled via secure vaults and minimal privilege, avoiding any hardcoded credentials. Finally, smooth integration with the local Codex agent workflows (and tools like Conductor) requires clear interfaces for task delegation and artifact exchange, so that cloud subagents’ outputs (code changes, data, etc.) can be reviewed and merged back seamlessly.  

# Findings  

## Orchestration Patterns  
Designing the coordination between a primary agent and cloud subagents requires choosing appropriate orchestration patterns:  

- **Sequential pipeline:** A chain of subagent tasks executed in order, where each agent’s output feeds the next. This is akin to a workflow pipeline – for example, using AWS Step Functions to orchestrate a series of Lambda functions, each performing a step ([www.cloudthat.com](https://www.cloudthat.com/resources/blog/building-multi-agent-systems-on-aws-for-smarter-and-collaborative-intelligence#:~:text=Sequential%20Chaining%20Pattern%20Agents%20execute,Lambda%20functions%20as%20agent%20wrappers)). This pattern is simple and ensures each sub-task completes before the next begins (useful for predetermined multi-step processes).  

- **Parallel delegation:** Multiple subagents run concurrently on independent subtasks, and their results are then aggregated. This pattern speeds up processing (e.g. having different agents analyze different data sources in parallel) ([www.cloudthat.com](https://www.cloudthat.com/resources/blog/building-multi-agent-systems-on-aws-for-smarter-and-collaborative-intelligence#:~:text=Parallel%20Execution%20Pattern%20Multiple%20agents,Amazon%20SQS%20for%20task%20distribution)). In AWS, one might invoke concurrent Lambda functions and use Amazon SQS or event buses to distribute and synchronize tasks.  

- **Hierarchical supervision:** A top-level *“supervisor”* agent oversees several worker subagents. The supervisor handles high-level planning, then delegates subtasks to specialized subagents and finally synthesizes their results ([www.cloudthat.com](https://www.cloudthat.com/resources/blog/building-multi-agent-systems-on-aws-for-smarter-and-collaborative-intelligence#:~:text=Hierarchical%20Orchestration%20Pattern%20A%20supervisor,Step%20Functions%20for%20hierarchical%20management)). This provides centralized control and state tracking. For instance, an orchestrator agent (possibly running on Amazon Bedrock or as a Step Functions workflow) can assign tasks to subagents (each a microservice or Lambda) and handle retries or fallback if one fails ([aws.amazon.com](https://aws.amazon.com/blogs/devops/multi-agent-collaboration-with-strands/#:~:text=A%20foundational%20step%20toward%20answering,agent%20systems)). The **Supervisor pattern** described by AWS exemplifies this: a central agent asynchronously delegates tasks and monitors subagents, providing structured coordination and fallback handling ([aws.amazon.com](https://aws.amazon.com/blogs/devops/multi-agent-collaboration-with-strands/#:~:text=A%20foundational%20step%20toward%20answering,agent%20systems)).  

- **Dynamic or event-driven orchestration:** Instead of a fixed sequence, the system can make runtime decisions on which subagent to invoke or when, based on the input or intermediate results. In this **dynamic routing** pattern, an agent uses conditional logic to route tasks (for example, an EventBridge rule or a Lambda router function picks an appropriate subagent based on task type) ([www.cloudthat.com](https://www.cloudthat.com/resources/blog/building-multi-agent-systems-on-aws-for-smarter-and-collaborative-intelligence#:~:text=Dynamic%20Routing%20Pattern%20Agents%20are,logic%20and%20Amazon%20EventBridge%20rules)). Additionally, an **event-driven** pattern lets agents trigger one another via events: subagents react to messages or changes, enabling loosely coupled collaboration ([www.cloudthat.com](https://www.cloudthat.com/resources/blog/building-multi-agent-systems-on-aws-for-smarter-and-collaborative-intelligence#:~:text=Event,SNS%2FAmazon%20SQS%20for%20message%20passing)). This is useful for long-running, asynchronous processes or scenarios where tasks need to run when certain conditions/events occur (for instance, a file upload event in S3 triggers a processing agent).  

- **Consensus or validation patterns:** In some cases, multiple subagents might attempt a task and the system chooses or merges results for higher reliability. For example, two agents could draft an answer and a third “arbiter” agent picks the best answer (voting/consensus) ([www.cloudthat.com](https://www.cloudthat.com/resources/blog/building-multi-agent-systems-on-aws-for-smarter-and-collaborative-intelligence#:~:text=Collaborative%20Consensus%20Pattern%20Multiple%20agents,Amazon%20DynamoDB%20for%20result%20aggregation)). This pattern can improve accuracy or safety (one agent’s work is checked by another). It adds overhead, so it’s used when the stakes (e.g. accuracy in financial decisions or code safety) warrant cross-verification.  

**Development note:** The orchestration can be implemented with cloud-managed services or custom logic. AWS Step Functions is a common choice for explicit workflows (especially sequential or parallel pipelines) because it manages state and integrates with other AWS services. Alternatively, a message queue (SNS/SQS or Kafka) enables event-driven or asynchronous agent activation. Each approach has trade-offs in complexity versus control. The key is to design orchestration so that adding or modifying subagents is easy (supporting the system’s scalability and maintainability) ([www.cloudthat.com](https://www.cloudthat.com/resources/blog/building-multi-agent-systems-on-aws-for-smarter-and-collaborative-intelligence#:~:text=Hierarchical%20Orchestration%20Pattern%20A%20supervisor,Step%20Functions%20for%20hierarchical%20management)) ([www.cloudthat.com](https://www.cloudthat.com/resources/blog/building-multi-agent-systems-on-aws-for-smarter-and-collaborative-intelligence#:~:text=Modularity%20and%20Maintainability%20Each%20agent,without%20affecting%20the%20entire%20system)). For example, treating each subagent as an independent microservice with a well-defined interface (as in the “Agent Squad” microservices pattern) can make the system modular and easier to update ([aws.amazon.com](https://aws.amazon.com/solutions/guidance/multi-agent-orchestration-on-aws/#:~:text=Agent%20Squad)).  

## Security Isolation  
Because AI subagents might execute arbitrary code or handle sensitive data, strong security isolation is paramount. Running code in the cloud does **not** automatically guarantee safety – it must be architected to contain any harmful or unintended actions. Key isolation practices include:  

- **Sandboxed execution environments:** Deploy each subagent in an isolated container or function environment to confine its effects. For instance, using AWS Lambda or Fargate with minimal privileges ensures the subagent’s code runs in a sandbox separate from the host system ([www.indiehackers.com](https://www.indiehackers.com/post/why-you-should-run-autogpt-within-a-virtual-container-f1f1caf556#:~:text=If%20you%20are%20using%20AutoGPT,files%20outside%20of%20the%20virtual)). This way, if an AI-generated script tries to delete files or make network calls, it can only impact its sandbox, not the broader application. In fact, running AI-generated code in a locked-down environment is *“highly advisable”* for safety ([www.indiehackers.com](https://www.indiehackers.com/post/why-you-should-run-autogpt-within-a-virtual-container-f1f1caf556#:~:text=If%20you%20are%20using%20AutoGPT,files%20outside%20of%20the%20virtual)). Any developer building an AI agent that can generate or run code should assume the code could be malicious or buggy and **must** be contained ([awsfundamentals.com](https://awsfundamentals.com/blog/sandboxing-with-aws-lambda#:~:text=Running%20untrusted%2C%20AI,scale%20when%20workloads%20spike%20unpredictably)). Cloud providers facilitate this by isolating serverless function instances at the infrastructure level – for example, AWS Lambda runs functions in micro-VMs to separate them from other workloads.  

- **Network and resource isolation:** Limit what each subagent can access. A best practice is deploying subagents in a **private network (VPC)** with no direct internet egress unless necessary. As an example, a team running untrusted code in Lambda put the function in a VPC with **no outbound internet**; it could only reach internal services (like S3 via a VPC endpoint) ([awsfundamentals.com](https://awsfundamentals.com/blog/sandboxing-with-aws-lambda#:~:text=Network%20Isolation%20with%20VPC)). This eliminated entire categories of threats by preventing the agent from calling unknown external websites or exfiltrating data ([awsfundamentals.com](https://awsfundamentals.com/blog/sandboxing-with-aws-lambda#:~:text=Network%20Isolation%20with%20VPC)). Similarly, consider using separate AWS accounts or at least separate IAM roles for different subagents or groups of subagents. This *blast radius* isolation means if one subagent is compromised, it cannot affect other parts of the system or access data it shouldn’t. Each subagent should run with the **least privileges** needed – e.g., if it only needs to read an S3 file and write a result, its IAM role should allow *only those actions on only that bucket*, and nothing more.  

- **Ephemeral execution & time limits:** Design subagents to be short-lived processes that terminate when the task is done. This reduces long-term exposure and makes it easier to reset state between runs. AWS Lambda by nature is ephemeral and can enforce a maximum runtime (up to 15 minutes, or shorter). Setting conservative timeouts is a simple but effective guardrail: for instance, configuring a Lambda subagent with a 30-second timeout ensures it can’t run endlessly ([awsfundamentals.com](https://awsfundamentals.com/blog/sandboxing-with-aws-lambda#:~:text=We%20built%20a%20custom%20Docker,no%20process%20monopolizes%20resources%20indefinitely)). In one use case, an average task took ~10 seconds, so a 30s timeout left buffer for legitimate variation but stopped any infinite loop from **“monopolizing resources indefinitely.”** ([awsfundamentals.com](https://awsfundamentals.com/blog/sandboxing-with-aws-lambda#:~:text=We%20built%20a%20custom%20Docker,no%20process%20monopolizes%20resources%20indefinitely)). If using container services or VMs for subagents, you can achieve similar limits by using Kubernetes job timeouts or AWS Batch job time limits, or by having the orchestrator terminate a job that exceeds expected duration.  

- **Input validation and monitoring:** Even in isolation, it’s wise to validate what the agent is allowed to do. For example, if a subagent is supposed to only transform text, add checks to ensure it’s not attempting to make OS-level changes or access disallowed files. Tools or frameworks that “guardrail” the AI’s actions (like restricting available APIs or using sandbox libraries) add another layer of defense inside the sandbox. Additionally, monitor subagents in real time – if one starts consuming unusually high CPU or memory, you might programmatically shut it down (many container orchestration platforms allow health and resource checks). The isolation should be multi-layered: OS-level (container/VM), network-level (firewalls/VPC), application-level (permission checks), and process-level (timeouts and resource limits). Adopting all layers significantly reduces the chances of a rogue AI action causing harm.  

## Scheduling  
Effective scheduling ensures that subagents run at the right moments and that resources are used efficiently. In a cloud-hosted multi-agent setup, scheduling considerations include:  

- **Event-driven triggers:** Utilize cloud events to invoke subagents when needed. For example, an EventBridge rule could trigger a subagent every night at 2 AM for a batch data processing task, or fire on-demand when a certain queue message arrives or a file appears in storage. By leveraging event-driven architecture, agents react to changes or requests in real time, rather than running constantly. AWS’s agent orchestration guidance highlights that these agents support *“event-driven or scheduled execution,”* meaning they can wake in response to events or on a timetable ([docs.aws.amazon.com](https://docs.aws.amazon.com/prescriptive-guidance/latest/agentic-ai-patterns/workflow-orchestration-agents.html#:~:text=,supervisors%2C%20collaborator%20agents%2C%20and%20tools)). This pattern is ideal for optimizing resource usage – the agent runs only when work is present or a schedule hits, which aligns with serverless principles.  

- **Coordinating multiple providers:** If offloading across providers (e.g., AWS and another cloud or on-prem), scheduling becomes about orchestrating across boundaries. One approach is to have a primary scheduler (for instance, an AWS Step Functions state machine or an orchestration service like Netflix Conductor/Temporal) that can call tasks on different platforms. For example, Step Functions could make an HTTPS call to an API on another provider to start a task there and wait for a callback or poll for completion. It’s important to align timeouts and retries across these boundaries – ensure that if an Azure function or GCP cloud run service is invoked by your AWS workflow, you have appropriate waiting and error handling. Alternatively, use a message bus that is accessible by services in both environments. The key is to have a **single source of truth** for scheduling so that tasks don’t conflict or duplicate across providers.  

- **Sequential vs. concurrent scheduling:** The orchestrator should decide when to run tasks in sequence versus in parallel. For workflows that have dependencies (output of A needed for B), it must schedule sequentially to enforce correct ordering ([www.cloudthat.com](https://www.cloudthat.com/resources/blog/building-multi-agent-systems-on-aws-for-smarter-and-collaborative-intelligence#:~:text=Sequential%20Chaining%20Pattern%20Agents%20execute,Lambda%20functions%20as%20agent%20wrappers)). Where tasks are independent, it should schedule in parallel to reduce overall latency ([www.cloudthat.com](https://www.cloudthat.com/resources/blog/building-multi-agent-systems-on-aws-for-smarter-and-collaborative-intelligence#:~:text=Parallel%20Execution%20Pattern%20Multiple%20agents,Amazon%20SQS%20for%20task%20distribution)). Cloud orchestration services support both modes (e.g., Step Functions can have parallel branches, or one Lambda can spawn multiple tasks in parallel and then join results). The **scheduling logic** should also include conditional branches – e.g., only run subagent C if subagent B’s result meets certain criteria. This conditional scheduling ties into the dynamic orchestration mentioned earlier.  

- **Throttling and concurrency control:** A critical scheduling concern is controlling how many subagents run at once, especially to avoid overwhelming resources or incurring high costs. If a user triggers 100 tasks simultaneously, do you really want 100 cloud functions to spin up? Often a safer design is to put tasks into a queue (like SQS) and have a limited number of worker subagents processing that queue. AWS Lambda allows setting *reserved concurrency* limits – for example, capping a particular Lambda to, say, 5 concurrent executions will queue any additional invocations beyond that ([moldstud.com](https://moldstud.com/articles/p-avoiding-common-pitfalls-in-cost-management-for-serverless-solutions#:~:text=,instead%20of%20allowing%20unlimited%20scale)) ([moldstud.com](https://moldstud.com/articles/p-avoiding-common-pitfalls-in-cost-management-for-serverless-solutions#:~:text=,resource%20exhaustion%20from%20load%20bursts)). This prevents sudden surges from exhausting budget or hitting service limits. Similarly, if using Kubernetes or ECS, you might set an autoscaling limit or a max number of parallel pods/tasks. By **smoothing out** the execution, you trade a bit of latency (tasks wait their turn) for stability and cost control. Each organization should decide the right concurrency level based on urgency of tasks versus cost risk.  

- **Resource-aware scheduling:** Not all subagents are equal – some might require GPU instances, more memory, or other special resources. A sophisticated scheduler will take resource requirements into account. For example, it could route a heavy ML inference task to an AWS SageMaker endpoint (for GPU acceleration) while sending a light text formatting task to a Lambda. If multiple tasks need the same limited resource (say a specialized model hosted on one instance), scheduling should queue or serialize those to avoid contention. AWS Batch or Kubernetes with resource requests can manage such scenarios by queuing jobs until the needed resources free up. In summary, scheduling should not only consider time, but also **capacity** – when resources are busy or expensive, schedule tasks in a way that optimizes their usage (possibly delaying non-urgent tasks to off-peak hours or lower-cost times, etc.).  

## Cost Controls  
Offloading AI tasks to the cloud introduces direct costs, so strong cost control mechanisms must be built-in by design. Our research uncovered several cost management strategies:  

- **Pay-per-use infrastructure:** Favor serverless and on-demand services so that cost scales with actual usage. Using AWS Lambda, for example, means you pay per invocation and execution time, and nothing when the agent is idle. This contrasts with running a server 24/7. Designing the subagents to be event-triggered and stateless helps here. AWS architects note that serverless architectures automatically adjust to demand and eliminate idle capacity costs, allowing you to *“scale AI interactions without [constant] infrastructure management.”* ([aws.amazon.com](https://aws.amazon.com/solutions/guidance/multi-agent-orchestration-on-aws/#:~:text=Scale%20AI%20interactions%20without%20infrastructure,management)) In practice, an agent that runs infrequently might incur only pennies on Lambda, whereas an always-on EC2 instance would cost dollars continuously. Therefore, partition tasks such that they can execute in short bursts on demand. For longer-running jobs, consider AWS Batch or spot instances to reduce cost, but still ensure they shut down when done.  

- **Budgets and guardrails:** Implement automated budget monitoring and enforcement. AWS Budgets can be configured to send alerts or trigger actions when spending approaches a limit ([aws.amazon.com](https://aws.amazon.com/blogs/compute/serverless-automated-cost-controls-part1/#:~:text=This%20improved%20agility%20also%20brings,freedom%2C%20but%20with%20financial%20constraints)) ([aws.amazon.com](https://aws.amazon.com/blogs/compute/serverless-automated-cost-controls-part1/#:~:text=1,state%20machine%20with%20the%20parameters)). For example, you might set a monthly budget for the multi-agent system (or per subagent). If 80% of the budget is hit, trigger an SNS notification to alert the engineering team. At 100%, you could automatically disable certain subagents or throttle their execution. AWS published a *“Serverless cost control”* blueprint that uses a budget alarm to invoke a Lambda which then, say, deactivates the ability to launch new tasks ([aws.amazon.com](https://aws.amazon.com/blogs/compute/serverless-automated-cost-controls-part1/#:~:text=1,state%20machine%20with%20the%20parameters)). This kind of **circuit breaker** ensures that a bug or misbehavior (like an agent stuck in a loop spawning tasks) doesn’t lead to a huge bill. It’s the cloud analog of setting a spending limit on a credit card. As AWS describes it: give developers freedom to use resources, but within financial guardrails to prevent surprises ([aws.amazon.com](https://aws.amazon.com/blogs/compute/serverless-automated-cost-controls-part1/#:~:text=This%20improved%20agility%20also%20brings,freedom%2C%20but%20with%20financial%20constraints)).  

- **Concurrency and rate limits:** As noted in scheduling, limiting how many tasks run in parallel is a direct way to cap cost surge. For instance, if each subagent execution costs X dollars at most, allowing unlimited concurrent runs could multiply costs unboundedly. By capping concurrency (using reserved concurrency on Lambdas or a fixed worker pool), you create an upper bound on burn rate ([moldstud.com](https://moldstud.com/articles/p-avoiding-common-pitfalls-in-cost-management-for-serverless-solutions#:~:text=,instead%20of%20allowing%20unlimited%20scale)). Similarly, implement rate limiting on how often certain expensive operations can be called. If a subagent uses an external API that charges per request or an LLM that bills per token, you may constrain it to a certain requests per minute or tokens per hour. These limits can be enforced in code or by using API gateway quotas etc.  

- **Optimize resource allocation:** Continuously review if the chosen compute and configurations are cost-optimal. One insightful finding was the **Lambda CPU-to-memory tradeoff**: AWS Lambda ties CPU share to the memory setting. In one case, to get sufficient CPU for a compute-heavy task, the function had to be allocated 2048 MB memory, even though it used only ~200 MB, meaning paying for ~10× memory not actually used ([awsfundamentals.com](https://awsfundamentals.com/blog/sandboxing-with-aws-lambda#:~:text=On%20AWS%20Lambda%2C%20CPU%20power,10x%20more%20memory%20than%20needed)). This is an example where switching to a different compute platform could save cost – e.g., an ECS/Fargate container with just the CPU and memory needed, or a cheaper instance if the workload is continuous. Thus, for each subagent, consider its usage pattern: short and spiky -> Lambda is good; long-running or CPU-bound -> maybe a container or VM pool is more cost-efficient. Also leverage pricing options: spot instances (for fault-tolerant batch jobs), Savings Plans or reserved instances for baseline steady workloads, etc., to reduce cost.  

- **Monitor and tune:** **Observability is intertwined with cost control** – by monitoring metrics like execution count, duration, and external API usage, you can identify cost hotspots. For example, CloudWatch or custom dashboards can show which subagent is called most and how that correlates to cost. Pay attention to often-overlooked costs too: data transfer fees, storage costs for artifacts, etc. Even a high-volume agent that transfers large datasets can incur notable data egress fees (AWS data transfer out can range from $0.09–$0.25/GB ([moldstud.com](https://moldstud.com/articles/p-avoiding-common-pitfalls-in-cost-management-for-serverless-solutions#:~:text=,in%20S3%20Standard%20exceeds%20%2423%2Fmonth))). If that becomes significant, you might refactor to keep data processing within the same region or use caching. Another scenario is an agent using an LLM API (like OpenAI) – those token costs can add up quickly. You should implement usage limits or at least alerts for such external calls. Crucially, have a process to regularly review the cloud billing reports for your subagents. Tag resources by agent or workflow where possible, so you can break down cost per subagent type ([moldstud.com](https://moldstud.com/articles/p-avoiding-common-pitfalls-in-cost-management-for-serverless-solutions#:~:text=,t%20automatically%20decommissioned)). This helps identify if, say, one feature’s agent is disproportionately expensive – maybe its logic can be optimized or run less often. In summary, treat cost like a monitored metric in your system, just as you’d monitor performance or errors.  

## Observability  
**“Traditional monitoring tools…fail to capture the nuanced behaviors of AI agents.”** ([medium.com](https://medium.com/%40acmopm/agentic-ai-observability-with-amazon-cloudwatch-transforming-enterprise-ai-monitoring-for-the-cc576b4258e8#:~:text=The%20emergence%20of%20agentic%20AI,behaviors%20of%20AI%20agents%20that)) Cloud-hosted subagents require advanced observability because their behaviors (and failure modes) can be far more complex than a standard microservice. We found best practices focusing on end-to-end visibility into what agents are doing and how the system is performing:  

- **Centralized logging and tracing:** All subagents should report log events to a central system, so you don’t end up playing “find the log” across many ephemeral containers. Services like Amazon CloudWatch Logs or Elastic Stack can collect logs from Lambda, ECS, etc. Include context in log entries – e.g., a correlation ID or session ID that ties a series of subagent calls to the original user request or to the parent agent’s invocation. This way, you can trace a full workflow across distributed components. Moreover, implement **distributed tracing** using tools like AWS X-Ray or OpenTelemetry. Tracing can record each subagent call as a span, along with metadata like what inputs it received and how long it ran. AWS has embraced OpenTelemetry for agent observability; their CloudWatch AI monitoring solution uses *“OpenTelemetry-native integration”* via the AWS Distro (ADOT) to instrument agents (including LLM calls) and send telemetry to CloudWatch ([medium.com](https://medium.com/%40acmopm/agentic-ai-observability-with-amazon-cloudwatch-transforming-enterprise-ai-monitoring-for-the-cc576b4258e8#:~:text=Core%20Infrastructure%20Components)) ([medium.com](https://medium.com/%40acmopm/agentic-ai-observability-with-amazon-cloudwatch-transforming-enterprise-ai-monitoring-for-the-cc576b4258e8#:~:text=AWS%20Distro%20for%20OpenTelemetry%20,statistics%2C%20and%20event%20loop%20monitoring)). By stitching together traces, you can visualize the entire path, which is invaluable for debugging and performance tuning. For example, you might discover that a particular step always adds a 5-second delay – trace data would pinpoint that.  

- **Metrics for internal agent behavior:** Beyond system metrics (CPU, memory, latency), AI agents benefit from custom metrics. For instance, track the number of iterations an agent goes through, the number of API/tool calls it makes, or tokens used per step. These can be treated as metrics and exported. *Token consumption* especially is a new crucial metric in LLM-based systems – a subagent stuck in a reasoning loop might consume millions of tokens. In fact, a lack of insight into token usage allowed an AI agent at one company to enter an infinite loop that *racked up $50k in API costs within 48 hours* ([medium.com](https://medium.com/%40acmopm/agentic-ai-observability-with-amazon-cloudwatch-transforming-enterprise-ai-monitoring-for-the-cc576b4258e8#:~:text=Real,monitoring%20and%20loop%20detection%20capabilities)). Proper observability (like monitoring token counts and detecting repeated patterns) could have flagged and stopped this. Consider implementing safeguards: e.g., if an agent uses >N tokens or runs >M steps for a single task, automatically halt and alert. CloudWatch’s recently introduced **Anomaly Detection** for AI agents aims to catch such aberrant behaviors in real-time ([medium.com](https://medium.com/%40acmopm/agentic-ai-observability-with-amazon-cloudwatch-transforming-enterprise-ai-monitoring-for-the-cc576b4258e8#:~:text=following%20features%3A)).  

- **Real-time alerts and dashboards:** Set up alerting on the metrics and logs discussed. For example, alert on error logs or on a sudden spike in executions. A CloudWatch dashboard (or third-party APM dashboard) tailored to your multi-agent system can show the status of each subagent type: e.g., how many tasks succeeded/failed in the last hour, current queue lengths, average latency, and cost per task. Also consider a **“heartbeat”** mechanism for long processes: if a subagent should finish in 5 minutes, but it’s still running at 15, alert or terminate it. Modern observability isn’t just reactive; it’s *proactive.* Having a *“conversational observability”* approach ([aws.amazon.com](https://aws.amazon.com/blogs/architecture/architecting-conversational-observability-for-cloud-applications/#:~:text=Architecture%20Blog%20aws,Amazon%20EKS%29%2C%20Amazon%20Elastic)) could mean you’re able to ask your monitoring system questions (some teams even experiment with ChatGPT analyzing logs). While that’s optional, at minimum ensure the operations team (or developers) have clear visibility into what the AI system is doing at all times, across cloud and local components.  

- **Audit logs and artifact tracking:** Especially when integrating with tools like Conductor (which is geared toward code changes), it’s important to keep thorough logs of what code changes were proposed or made by AI subagents. If a cloud agent writes to a repository or file store, log that commit or file path. This creates an audit trail so that any destructive or incorrect change is traceable back to its origin. In highly regulated or sensitive contexts, you might even log the prompts and responses of the LLM agents for later review (ensuring you don’t log secrets). AWS’s Generative AI Observability features include *“audit trail capabilities”* for agent actions ([medium.com](https://medium.com/%40acmopm/agentic-ai-observability-with-amazon-cloudwatch-transforming-enterprise-ai-monitoring-for-the-cc576b4258e8#:~:text=Amazon%20Bedrock%20AgentCore%20Observability%2C%20introduced,com%2Fevents%2Fsummits%2F%2C%20provides%20the%20following%20features)), indicating the trend toward logging not just system metrics but semantic actions (like which tool an agent invoked, or which knowledge base it queried). Such detailed observability helps in debugging complex sequences and also provides transparency (useful if human oversight or review is required for certain AI-driven decisions).  

- **Multi-provider visibility:** Given the question context of “across providers,” ensure your observability solution can aggregate data from all platforms involved. You might need to push logs/metrics from a non-AWS provider into a central system. Many teams use platform-agnostic tools (like Datadog, New Relic, or an ELK stack) to gather data from AWS, GCP, etc., in one place. AWS CloudWatch can also receive custom metrics/traces, so with some work you could feed it data from external systems. The end goal is to **avoid blind spots** – you don’t want an agent on another cloud doing something costly or failing silently because your monitoring was only looking at AWS. Design a unified observability strategy from the start, even if it’s as simple as ensuring every agent reports a heartbeat and status to a common dashboard.  

## Secrets Management  
Handling secrets (API keys, credentials, tokens) in a distributed agent system requires strict practices to avoid leaks or unauthorized access. Best practices for secrets in this context include:  

- **Centralized secret storage:** Do **not** hardcode secrets or put them in code repositories. Instead, use a service like AWS Secrets Manager or HashiCorp Vault to store all sensitive keys. AWS Secrets Manager, for example, keeps secrets encrypted (using KMS) at rest and sends them securely (TLS) when needed ([medium.com](https://medium.com/codex/securing-your-secrets-with-aws-secrets-manager-best-practices-and-tips-b6be94fc7cd6#:~:text=AWS%20Secrets%20Manager%20provides%20a,grained%20access%20control%20to%20secrets)). By centralizing, you reduce the attack surface (no scattered config files with passwords) and gain management features. Each subagent that needs a secret (say an API key to call an external service) can be granted read access to just that one secret. The subagent fetches it at runtime from Secrets Manager. This way, even if the code is visible, the actual secret value is not present except in memory at execution time.  

- **Least privilege and segmentation:** Use IAM roles and policies to ensure only the intended agent or service can access a given secret ([medium.com](https://medium.com/codex/securing-your-secrets-with-aws-secrets-manager-best-practices-and-tips-b6be94fc7cd6#:~:text=2,source%20code%20or%20configuration%20files)). For instance, if one subagent needs a database password, only that Lambda (identified by its role) can retrieve that secret from Secrets Manager. It’s good practice to isolate secrets by context too – e.g., dev, test, prod environments have separate secrets and perhaps separate accounts for secrets. That prevents a dev agent from accidentally getting prod credentials. Also, do not give a single monolithic role access to *all* secrets unless absolutely necessary; break it down so each component has a narrow scope. AWS IAM and Secrets Manager allow very granular access control, use it to enforce this principle of least privilege ([docs.aws.amazon.com](https://docs.aws.amazon.com/secretsmanager/latest/userguide/best-practices.html#:~:text=Limit%20access%20to%20secrets)).  

- **Secret injection and usage:** Integrate secret retrieval into your deployment/workflow such that secrets are loaded securely into the agent’s runtime. For AWS Lambda, you might use an environment variable that the Lambda runtime loads from Secrets Manager on startup (there’s a Secrets Manager Lambda extension that can cache and supply secrets at container startup). For containerized subagents, you can use AWS ECS secrets integration or Kubernetes secrets to mount the secret as an env var or file. The key is to avoid ever printing the secret or exposing it. Many managed services will redact secrets in logs if you follow their mechanism (for example, if a Lambda pulls from Secrets Manager, the AWS SDK might ensure it’s not logged; but if you accidentally log the variable, that’s on you). So be cautious in code to never log or echo secrets.  

- **Automatic rotation:** Enable secret rotation policies for long-lived credentials. AWS Secrets Manager can auto-rotate many types of secrets (database passwords, certain API keys) on a schedule you define – for instance, rotate every 30 days, or even every few hours for highly sensitive ones ([medium.com](https://medium.com/codex/securing-your-secrets-with-aws-secrets-manager-best-practices-and-tips-b6be94fc7cd6#:~:text=1,access%20credentials%2C%20leverage%20IAM%20roles)). If rotation is enabled, design your subagents to fetch the secret value at runtime (or just-in-time) rather than caching it indefinitely, so they always use the latest. Also handle the brief period during rotation when multiple versions of a secret exist (Secrets Manager uses staging labels to manage current/previous secrets during rotation). The benefit is that even if a secret leaked, it would become invalid after rotation. Additionally, when an agent stops, it should not persist any secrets to disk. If it wrote to disk (say in a temp file), ensure that gets wiped when the container is destroyed.  

- **Cross-provider secret sharing:** If your workflow spans providers, you have a choice: use a separate secret manager on each (AWS SM, GCP Secret Manager, etc.) or use one as the primary. Some setups use a central Vault that all agents, wherever they run, can reach (with proper auth). Others keep secrets local to each cloud and pass tokens as needed. For example, the local orchestration might fetch an API key from AWS Secrets Manager and then pass it in an invocation to a GCP cloud function. That’s viable but be careful to transmit it securely (HTTPS and maybe short-lived tokens instead of raw creds). A safer pattern is issuing temporary credentials. AWS’s STS tokens or Azure’s MSI can provide temp credentials that expire, which are safer to hand off. In any case, audit the flows of secrets: know exactly which agent gets which secret. Monitor secret usage – AWS Secrets Manager can log access events; you can set up alerts if a secret is accessed at an unusual time or by an unexpected entity ([docs.aws.amazon.com](https://docs.aws.amazon.com/secretsmanager/latest/userguide/best-practices.html#:~:text=Secrets%20Manager%20enables%20you%20to,For%20more%20information%2C%20see)). This can catch misconfigurations or potential compromise early.  

- **Infrastructure secrets:** Don’t forget that not just API keys but also things like database connection strings or encryption keys are secrets. Use the same rigor for those. If your subagent needs to connect to a database, don’t bake the DB password in an AMI or user-data; store it in Secrets Manager and retrieve on startup. Use encryption keys via KMS where possible (instead of storing raw keys, call KMS API to encrypt/decrypt as needed). Essentially, **treat anything confidential as a secret** and handle accordingly. As one expert noted, a huge portion of breaches come from poor secret management (e.g., keys in GitHub repos) – so this area, while not glamorous, is critical to the overall security of AI workflows ([us.nttdata.com](https://us.nttdata.com/en/blog/2020/may/blog-detailten-best-practices-for-managing-application-secrets-in-aws#:~:text=DATA%20us,Microsoft%20program%20manager%20as%20quoted)).  

## Integration with Local Workflows (Codex & Conductor)  
Integrating cloud subagents into local development workflows (like a local Codex agent environment or the Conductor app) requires thoughtful design so that the offloading feels seamless. Key integration considerations and best practices are:  

- **Clear task interface:** Define a clean interface through which the local agent (Codex) delegates a job to the cloud. This could be a function call that invokes an API endpoint for the subagent, or enqueuing a message that a cloud worker listens to. The local workflow should not need to know **how** the cloud does it, just how to request it. For example, within Conductor, a developer might click “Run Tests (Cloud)” and behind the scenes this triggers an AWS Lambda to run tests. Conductor/the local agent would then wait for a response or poll a status. Using an asynchronous pattern is usually best – the local agent can continue with other work (or at least remain responsive) while the cloud subagent works, then handle the result when ready. Technologies like Webhooks, long-polling, or leveraging Conductor’s own messaging (if available) can help the cloud agent signal completion. The **contract** should include what input context the subagent needs (e.g., code snippet, data pointer) and what output it will produce (e.g., “I will return a JSON with results or an S3 link to artifacts”). Designing this API clearly up front will make integration much smoother.  

- **Context and artifact management:** Since the cloud subagent might not have direct access to the local state, you need a strategy to supply it the necessary context. For instance, if the subagent is to analyze the codebase, you might zip the relevant project files and upload to S3 for the subagent to retrieve, or the subagent might pull the latest code from a git repo. In Conductor’s case, each agent works on an isolated copy of the codebase ([docs.conductor.build](https://docs.conductor.build/#:~:text=Conductor%20is%20a%20Mac%20app,changes%2C%20all%20in%20one%20place)); a cloud agent would likewise need an isolated snapshot. One approach is to containerize the code (like create a Docker image or bundle) and send it along. Regardless, minimize context to just what’s needed (for performance and security). After the subagent finishes, there’s the question of **artifacts**: results, files, or code changes it produces. Best practice is to have the cloud agent store artifacts in a known location (a database, S3 bucket, or repository) rather than pushing directly into the local system – this allows for a checkpoint and possibly a review step. For example, a cloud “code fix” agent could generate a patch or diff and place it in S3, and then the local Conductor UI can display that diff to the developer for approval and merging. Conductor is built around reviewing and merging AI-generated code changes in one place ([docs.conductor.build](https://docs.conductor.build/#:~:text=Conductor%20is%20a%20Mac%20app,changes%2C%20all%20in%20one%20place)), so ensure the outputs from cloud agents can be presented similarly (e.g., as a unified diff or a new branch in the repo). It might be useful to tag artifacts with the agent name and task ID for traceability (so you know which agent produced which artifact).  

- **Workflow and state synchronization:** If the local agent and cloud subagents are part of one logical workflow, keep their state in sync. This might involve the cloud subagent updating a state machine or database that the local side can query. For example, a local agent might initiate a cloud task and record an “activity ID”. The cloud subagent, once done, updates a record (or sends a callback) saying “activity ID X – completed with result Y”. The local agent can then pick that up and continue. Designing a small state repository (even just a DynamoDB table or a status file) can facilitate decoupling: the local side writes an entry “task pending” and cloud side updates it to “done” with results. Conductor artifacts likely include some workflow metadata, so tie into that if possible (maybe Conductor can be extended to have a notion of an external task with a status). **Error handling** is part of this synchronization: if the cloud task fails or times out, it should record that status and perhaps error details. The local workflow must then detect that and handle it (maybe retry or notify the user). A robust design accounts for partial failures – e.g., if the local system goes offline after dispatching to cloud, the cloud might complete and hold results waiting; or if the cloud fails, the local should not hang forever. Timeouts and callbacks are thus essential to implement.  

- **Development and testing workflow:** During development of these agents, it’s beneficial to be able to test end-to-end without deploying to production cloud every time. The **hybrid development approach** some teams have used is instructive: they run the main application locally, but connect it to a cloud dev environment for the subagent ([awsfundamentals.com](https://awsfundamentals.com/blog/sandboxing-with-aws-lambda#:~:text=The%20Hybrid%20Development%20Workflow)). For instance, you could have a dev-stage Lambda or an ECS service for the subagent running in AWS, and your local agent calls that. This was exactly the approach taken by one group, who found that treating the cloud component as a black-box service even in development allowed them to iterate quickly on the local code while testing against real cloud execution ([awsfundamentals.com](https://awsfundamentals.com/blog/sandboxing-with-aws-lambda#:~:text=The%20Hybrid%20Development%20Workflow)). The lesson is to **not** try to fully simulate the cloud subagent locally (that can be complex and inaccurate); instead, use the real thing in a sandbox environment. Tools like localstack (which simulates AWS services) exist, but for something as dynamic as AI code execution, nothing beats the real environment for testing. Therefore, invest in a good continuous integration pipeline and possibly automated tests that spin up the cloud agent with sample tasks to verify integration.  

- **User experience in Conductor:** From the end-user perspective (e.g., a developer using Conductor), the integration should ideally be intuitive. They might not even need to know a task ran on AWS – aside from maybe a slight delay or a label “(cloud)”. Aim to integrate status updates into the UI: e.g., “Running tests in AWS… (spinner)” and then “Tests passed, view results” or “Tests failed, click to see logs.” This means piping cloud logs/errors back to the local UI. You could set up a mechanism for the cloud agent to stream logs or periodically send log chunks that Conductor can display in real-time, similar to how one would see logs if running tests locally. Achieving this might involve a WebSocket or periodic polling of a log store. If Conductor supports plugin development, one could create a plugin that interfaces with the cloud (polling AWS CloudWatch Logs for that function’s output, for instance) and feeds it into the app. Additionally, consider security of this integration: the local tool should not expose cloud credentials directly. Possibly use an intermediary service or limited-scope API keys for the local app to fetch results (for example, generate a pre-signed S3 URL for the artifact so Conductor can download it without needing AWS creds).  

- **Maintaining reproducibility:** When cloud and local environments mix, ensure that results are reproducible and environments are aligned. If a subagent uses a different Python version or library set in AWS, its output might not work on local. Strive to use similar environments – perhaps containerize the local agent environment as well, or use the same base Docker image for local and cloud runs when possible. Conductor giving each Claude agent an isolated copy of the codebase is about isolation; you may extend that concept such that the cloud agent runs on the same code snapshot. If the local code changes (e.g., user edits), consider how to sync those changes to the cloud agent if it needs the latest. One strategy is to require a commit of changes before the cloud agent runs, ensuring cloud always works off a committed state (which also aids audit/versioning). These process details will make the integration smoother and less error-prone.  

In summary, integrating cloud subagents with local workflows demands careful API and workflow design to hide the complexity from users, while maintaining transparency for debugging. By sending well-scoped tasks to the cloud, securely handing over context (and secrets), and receiving back results and logs, you can leverage the cloud’s power from within a local development loop. The combination of local Codex agents (for quick, interactive tasks) and cloud offloaded agents (for heavy lifting or sandboxed execution) can be very powerful – as long as they’re orchestrated in a coherent way. The **Conductor** tool, designed to coordinate multiple coding agents, can be extended to include cloud-run agents by treating them as just another kind of agent whose “workspace” happens to be remote. The principles above ensure that this extension preserves security, observability, and efficiency.  

# Recommendations  
Based on the above findings, here are concrete design recommendations for building cloud-hosted AI subagents with AWS offloading, integrated into local agent workflows:  

1. **Adopt a suitable orchestration pattern:** Use a centralized orchestrator (e.g., a supervisor agent with AWS Step Functions or a queue system) to manage subagent workflows. Choose sequential vs. parallel execution deliberately – chain dependent tasks, but run independent tasks concurrently to improve throughput. For dynamic scenarios, implement event-driven triggers or conditional logic so the system can flexibly assign tasks to the right subagent at runtime. Ensure the orchestrator includes retry and fallback logic to handle subagent failures gracefully.  

2. **Enforce strong isolation for subagents:** Sandbox each cloud subagent in its own minimal-privilege environment. This means running code in isolated AWS Lambdas or containers with locked-down permissions (IAM roles) and network access. Remove internet access for agents unless necessary, and even then tightly restrict egress (consider allowlisting required domains or using VPC endpoints for cloud services). Use short execution timeouts and resource limits to contain runaway processes. By isolating at multiple levels (compute, network, and permissions), you protect both the local system and cloud environment from malicious or errant agent behavior.  

3. **Implement scheduling and concurrency controls:** Leverage AWS scheduling tools to invoke agents on schedules or events (e.g., EventBridge for cron-like triggers, S3 events, DynamoDB streams, etc.). The system should also throttle how many agents run at once – for example, cap Lambda concurrency or queue requests – to prevent spikes that could overload systems or budgets. Design your workflows to be largely asynchronous, allowing the orchestrator to juggle many tasks and scale out when needed, but within preset limits. Concurrency limits and careful scheduling will ensure the solution scales smoothly without surprises.  

4. **Embed cost management into the design:** Put cost guardrails in place from day one. Tag and monitor all cloud resources by subagent or project, and set up AWS Budgets alerts for cost anomalies. Make sure any auto-scaling has sane upper bounds. Use pay-as-you-go services (Lambda/Fargate) by default, and turn off long-running resources when they’re idle. Where possible, prefer built-in managed services over custom infrastructure – they often optimize resource use internally (for example, Lambda optimizes instance sharing under the hood). Also, regularly review usage: if a certain subagent is very expensive, investigate optimizing its code or switching it to a more cost-efficient runtime (such as moving from Lambda to a batch job on a cheaper instance). Essentially, treat cloud execution cost as a key metric to optimize, just like latency or accuracy in an AI system.  

5. **Build comprehensive observability and alerting:** Equip each subagent with logging and monitoring from the start. Use a centralized log system (CloudWatch or similar) and ensure you can trace an entire request across subagents (propagate trace IDs). Monitor both system metrics (CPU, memory, execution time) and AI-specific metrics (like tokens used, number of steps, etc.). Set up automated alerts for unusual behavior – e.g., if an agent suddenly executes far more times than normal or runs much longer than expected. Given the complexity of multi-agent systems, consider using distributed tracing and even AI-tailored monitoring tools (AWS’s CloudWatch ServiceLens or Bedrock agent monitoring) to get insight into agent reasoning paths. Quick detection of anomalies will not only save costs but also prevent small glitches from compounding into bigger failures.  

6. **Manage secrets and credentials securely:** Store all secrets in AWS Secrets Manager (or a similar centralized vault) and **never** hardcode them. Subagents should obtain credentials at runtime and only if their IAM role permits it. Follow least-privilege principles: each agent or component only gets access to the specific secrets it truly needs. Enable automatic rotation for long-lived secrets (database passwords, API keys) and ensure your agents can handle rotated credentials (e.g., by always pulling the latest from the vault). In integration code, take care to avoid logging any secret values. Treat cloud credentials (API keys, tokens) with the same care on the local side – e.g., if Conductor needs an AWS API key to trigger a Lambda, limit its permissions and possibly scope it to dev/test vs. prod.  

7. **Integrate cloud agents seamlessly into local workflows:** Design the interaction such that offloading a task to the cloud is as straightforward as a local function call from the perspective of the local agent. Use clear APIs or message formats for requests and results. Provide feedback in the local user interface (e.g., status updates, progress) when a cloud task is running, so the user knows it’s in progress. When the cloud subagent finishes, automatically bring the results or artifacts into the local context – for example, download result files or apply code patches (but maybe gated by a review). Maintain a consistent format for artifacts so that whether a change is made by a local Codex agent or a cloud agent, the developer can review it in the same way. Moreover, invest in testing this integration: simulate various failure modes (cloud agent timeout, network issue, etc.) and ensure the local workflow handles them (perhaps by falling back to a simpler method or prompting the user).  

8. **Use a hybrid development and deployment strategy:** In development, run and debug your primary agent or application locally, but call out to cloud-hosted subagents in a controlled dev environment. This allows you to iterate quickly on logic and still test the end-to-end cloud offloading. For deployment, automate the provisioning of subagents (Infrastructure as Code for AWS resources, CI/CD for agent code) to avoid configuration drift between local and cloud. Keep environment parity as much as possible (same library versions, similar configuration) between local and cloud execution to minimize surprises. By treating the cloud components as true services (even in dev), you encourage a modular design and catch integration issues early.  

By following these recommendations – robust orchestration, strong isolation, careful scheduling, cost vigilance, thorough observability, secure secret handling, and intuitive integration – you can confidently extend a local AI agent system with powerful cloud-based subagents. The result will be a scalable, secure, and maintainable multi-agent architecture that leverages the best of both local and cloud resources.  

# Sources  

- AWS DevOps Blog – *“Multi Agent Collaboration with Strands”* (AWS Developer Productivity Blog)【3】 – Describes the Supervisor and Arbiter patterns for coordinating multiple AI agents, highlighting the need for orchestration, dynamic agent management, and the benefits of a centralized supervisor in AWS.  
- CloudThat Blog – *“Building Multi Agent Systems on AWS for Smarter and Collaborative Intelligence”*【20】 – Outlines key orchestration patterns (sequential, parallel, hierarchical, dynamic, etc.) with AWS implementations, as well as benefits and challenges of multi-agent orchestration.  
- IndieHackers post – *“Why you should run AutoGPT within a virtual container”*【12】 – Emphasizes running autonomous AI agents in isolated environments (Docker/VM) to protect the host system, a rationale applicable to cloud-hosted agent isolation.  
- Piotr Migdał (awsfundamentals.com) – *“Running Untrusted Code Safely at Scale with AWS Lambda”*【26】 – Case study of using AWS Lambda (with container images and VPC isolation) to safely execute AI-generated code. Provides insights on security isolation (network-locked down Lambda), Lambda timeout guards, and notes on Lambda cost/performance trade-offs.  
- AWS Compute Blog – *“Serverless Automated Cost Controls, Part 1”*【15】 – Presents an architecture using AWS Budgets, SNS, Lambda, and Step Functions to enforce cost limits (“freedom with guardrails”). Relevant to setting up automated budget alerts and actions to prevent overruns.  
- Moldstud Engineering – *“Avoiding Cost Management Mistakes in Serverless Solutions”*【32】 – Shares best practices for serverless cost optimization, including using reserved concurrency to prevent over-scaling, and tagging/monitoring to attribute costs.  
- Medium (Anand Mehta) – *“Agentic AI Observability with Amazon CloudWatch”*【17】 – Discusses the challenges of observing autonomous AI agents and AWS’s approach with CloudWatch and OpenTelemetry. Notably mentions the need for monitoring dynamic agent behaviors, token usage, and an example of a costly infinite loop that observability could catch.  
- Medium (Roman Ceresnak, CodeX) – *“Securing Your Secrets with AWS Secrets Manager: Best Practices and Tips”*【30】 – Summarizes best practices for managing secrets on AWS, including regular rotation, least privilege access via IAM, and using IAM roles instead of hardcoding credentials.  
- Conductor Documentation – *Conductor.app Welcome and Codex integration*【9】【7】 – Provides context on the Conductor tool (isolating multiple Claude or Codex agents on a codebase). Demonstrates the environment where local multi-agent workflows occur, which our integration design addresses. (Used for understanding integration context rather than direct quotes.)  

# Open Questions  

- **Multi-cloud orchestration** – If extending beyond AWS to other providers, what is the best way to orchestrate and monitor across clouds? It remains a design question whether to use a single orchestrator (e.g., AWS Step Functions calling out to others, or an external tool like Temporal) or segregate workflows per cloud. A unified control plane for multi-cloud agents is ideal for simplicity, but building that introduces complexity – further exploration is needed to determine the most resilient approach.  

- **Optimizing context transfer** – How to efficiently provide the cloud subagents with the necessary context (code, data) without incurring huge overhead? For example, shipping an entire codebase to AWS on each invocation could be slow and costly. Open questions include whether techniques like shared network file systems, incremental diffs, or caching of context on the cloud side can be employed to reduce data transfer. The solution may depend on the use case, and finding the right balance between up-to-date context and efficiency will require experimentation.  

- **Agent governance and safety** – While we can sandbox and monitor agents, ensuring they always behave as intended is an ongoing challenge. How do we detect an agent that is producing subtly incorrect output or not following business rules? This leans into AI alignment and governance territory. Possible ideas include incorporating verification agents (as in a consensus pattern) or human-in-the-loop review for certain actions. Defining which tasks should require human approval (especially in a coding context, to avoid bad code changes) is an open policy question. As these multi-agent systems evolve, establishing trust and verification mechanisms (beyond basic isolation) will be important to explore.  

- **Integration with Conductor’s roadmap** – As Conductor (and similar developer tools) advance, they may introduce native ways to plug in cloud resources or manage artifacts. An open question is how the evolving features of Conductor might handle remote agent outputs – for example, will Conductor provide an API to programmatically add a code suggestion from a cloud agent into the review UI? Currently, our integration relies on custom handling; in the future, closer collaboration with the Conductor developers or using their extension points might yield a more elegant integration. It’s worth tracking Conductor’s updates to align our design with its best practices or plugin architecture.

## Research Question

Design guidance for cloud-hosted AI subagents with AWS offloading across providers: orchestration patterns, security isolation, scheduling, cost controls, observability, secrets, and best practices for integrating with local Codex agent workflows and Conductor artifacts.